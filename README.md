# MultiPhysio-HRC: Multimodal Physiological Signals Dataset for Industrial Human-Robot Collaboration

> Companion GitHub repository for the paper **â€œMultiPhysio-HRC: Multimodal Physiological Signals Dataset for Industrial Human-Robot Collaboration.â€**  
> This repo hosts **code, preprocessing pipelines, feature extraction, and baseline models** to reproduce results from the paper.

<p align="center">
  <a href="#citation"><img alt="Cite this" src="https://img.shields.io/badge/Cite-this-blue"></a>
  <a href="#getting-started"><img alt="Python" src="https://img.shields.io/badge/Python-3.9%2B-blue"></a>
  <a href="#license"><img alt="License" src="https://img.shields.io/badge/Code-MIT-blue"></a>
  <a href="https://doi.org/XXXX"><img alt="Dataset" src="https://img.shields.io/badge/Dataset-Zenodo-brightgreen"></a>
  <img alt="Status" src="https://img.shields.io/badge/Status-Active-success">
</p>

---

## ðŸ“– Overview
**MultiPhysio-HRC** is a multimodal dataset and toolkit for **mental-state estimation** in **industrial Human-Robot Collaboration (HRC)**.  

This repository provides:
- Loaders and preprocessing for physiological, EEG, voice, and AU data.
- Feature extraction pipelines for all modalities.
- Baseline regression and classification models with LOSO-CV evaluation.
- Example notebooks to explore the dataset and reproduce paper results.

---

## ðŸ“‚ Repository Structure

```
.
â”œâ”€ paper/                          # Cameraâ€‘ready or preprint PDF, figures (optional)
â”œâ”€ docs/
â”‚  â”œâ”€ dataset_overview.md          # Modalities, tasks, questionnaires, ethics
â”‚  â”œâ”€ data_schema.md               # File formats, splits, naming, timestamps
â”‚  â””â”€ benchmarks.md                # Baseline setups & expected metrics
â”œâ”€ src/
â”‚  â”œâ”€ dataprep/                    # Loading, syncing, cleaning
â”‚  â”œâ”€ features/                    # Physio, EEG, voice, AUs feature extraction
â”‚  â”œâ”€ models/                      # Baselines (RF/AB/XGB), utils
â”‚  â””â”€ eval/                        # Metrics, LOSO CV, reporting
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_quicklook.ipynb           # Explore a subject & modalities
â”‚  â”œâ”€ 02_extract_features.ipynb    # Endâ€‘toâ€‘end feature extraction
â”‚  â””â”€ 03_train_baselines.ipynb     # Reproduce results from the paper
â”œâ”€ examples/
â”‚  â””â”€ minimal_pipeline.py          # Scripted endâ€‘toâ€‘end run
â”œâ”€ requirements.txt                # Python deps
â”œâ”€ pyproject.toml                  # (optional) for modern builds
â”œâ”€ CITATION.cff                    # Paper metadata (fill in DOI when available)
â””â”€ README.md                       # You are here
```

> **Tip:** If you keep raw data outside the repo, set `MULTIPHYSIO_HRC_DATA` env var to the dataset root to avoid passing paths around.

---

## Getting Started
### 1) Install
```bash
# create a clean env (conda or mamba recommended)
conda create -n mphrc python=3.10 -y
conda activate mphrc

# install dependencies
pip install -r requirements.txt
```

### 2) Download the dataset
- Visit **https://tinyurl.com/MultiPhysio-HRC** and follow the instructions to obtain access and download files.
- Keep the raw data in a folder of your choice and set:
```bash
export MULTIPHYSIO_HRC_DATA=/path/to/MultiPhysio-HRC
```

### 3) Sanityâ€‘check a subject
```bash
jupyter lab  # then open notebooks/01_quicklook.ipynb
```

---

## Data Schema
```
MultiPhysio-HRC/
â”‚
â”œâ”€â”€ physiological_data/
â”‚   â”œâ”€â”€ filtered/                # Preprocessed signals
â”‚   â”‚   â”œâ”€â”€ subj1/
â”‚   â”‚   â”‚   â”œâ”€â”€ task1.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ task2.csv
â”‚   â”‚   â”‚   ...
â”‚   â”‚   â””â”€â”€ subj2/
â”‚   â”‚       â”œâ”€â”€ task1.csv
â”‚   â”‚       â”œâ”€â”€ task2.csv
â”‚   â”‚       ...
â”‚   â”‚
â”‚   â””â”€â”€ raw/                     # Raw signals as acquired
â”‚       â”œâ”€â”€ subj1/
â”‚       â”‚   â”œâ”€â”€ task1.csv
â”‚       â”‚   â”œâ”€â”€ task2.csv
â”‚       â”‚   ...
â”‚       â””â”€â”€ subj2/
â”‚           â”œâ”€â”€ task1.csv
â”‚           â”œâ”€â”€ task2.csv
â”‚           ...
â”‚
â”œâ”€â”€ features/                    # Extracted features and labels
â”‚   â”œâ”€â”€ aus_data.csv
â”‚   â”œâ”€â”€ bio_features_60s.csv
â”‚   â”œâ”€â”€ eeg_features_5s.csv
â”‚   â”œâ”€â”€ nlp_embeddings.csv
â”‚   â”œâ”€â”€ speech_features.csv
â”‚   â””â”€â”€ labels.csv
|
â””â”€â”€ README.md
```
- **Windows:** Physio features on 60 s windows; EEG on 5 s windows; AUs at 2 fps.

---

## Reproducing the Paper Baselines
**Endâ€‘toâ€‘end (script):**
```bash
python examples/minimal_pipeline.py \
  --data $MULTIPHYSIO_HRC_DATA \
  --modality physio \
  --task regression --label STAI \
  --cv loso --report out/report_physio_stai.json
```

**Notebooks:**
1. `02_extract_features.ipynb` â€“ computes features for Physio/EEG/Voice/AUs.
2. `03_train_baselines.ipynb` â€“ trains RF / AdaBoost / XGBoost for regression & 3â€‘class classification (Low/Med/High) based on perâ€‘subject zâ€‘like thresholds.

**Models:** RandomForest, AdaBoost, XGBoost. Evaluation uses **Leaveâ€‘Oneâ€‘Subjectâ€‘Out (LOSO)**. Features & labels are minâ€“max normalized withinâ€‘subject as in the paper.

---

## Results (from the paper)
- **Regression (STAIâ€‘Y1 & NASAâ€‘TLX):** Physiological features yield the **lowest RMSE**, stronger than EEG and Voice.
- **3â€‘Class Classification (Stress & Cognitive Load):** Physiological features achieve the **highest F1**, with EEG close behind for cognitive load; Voice trails Physio/EEG.

> See `docs/benchmarks.md` for expected ranges and how we compute the Low/Med/High bins per subject.

---

## FAQ
**Q: How do I get access to raw videos or robot logs?**  
A: See the dataset page. Some assets may require additional request/agreements.

**Q: Are there readyâ€‘made splits?**  
A: We default to **Leaveâ€‘Oneâ€‘Subjectâ€‘Out**. Utility functions can generate stratified splits by task/condition.

---

## Citation
If you use **MultiPhysioâ€‘HRC** or this code, please cite the paper:

```bibtex
@article{MultiPhysioHRC2025,
  title   = {MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration},
  author  = {Bussolan, Andrea and Baraldo, Stefano and Avram, Oliver and Urcola, Pablo and Montesano, Luis and Gambardella, Luca Maria and Valente, Anna},
  year    = {2025},
  journal = {TBD},
  volume  = {TBD}, number = {TBD}, pages = {TBD},
  doi     = {TBD},
}
```

---

## Acknowledgments & Funding
- **Horizon Europe â€” FLUENTLY** (Grant **101058680**)
- **Eurostars â€” !2309â€‘Singularity**
- We thank all participants and the technical staff who supported the acquisition campaign.

---

## Ethics & License

This dataset was collected under institutional ethical approval (SUPSI), with informed consent from all participants.
- **Code:** Licensed under the [MIT License](https://opensource.org/licenses/MIT).  
- **Dataset:** Released under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).  

---

## Contact
- Lead contact: **andrea.bussolan@supsi.ch**
- Issues & questions: please open a GitHub issue.

---

*Maintainers:* Andrea Bussolan, Stefano Baraldo.